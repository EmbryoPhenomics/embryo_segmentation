{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f7a7533-b204-43ff-8a79-be672b2b9b5d",
   "metadata": {},
   "source": [
    "# Training notebook for semantic segmentation of embryos in microscopy images\n",
    "\n",
    "A notebook outlining the training procedure using a variety of popular semantic segmentation models such as UNet and DeepLab V3. Because this application only includes single instances of embryos in any given microscopy images, we will frame this as a binary segmentation task. Though if your application includes multiple instances of embryos in a given image, you will need to instead make use of instance segmentation models such as Mask-RCNN.\n",
    "\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "The following are required dependencies for this script. We also set up mixed precision training for the speedup it provides in training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c86a6-6a23-4a64-a9db-32b5f449d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import vuba\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import vuba\n",
    "from tensorflow import keras\n",
    "import re\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from natsort import ns, natsorted\n",
    "import pandas as pd\n",
    "\n",
    "from seg_models import build_model\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Parameters ----------------------------------------------------------\n",
    "batch_size = 32\n",
    "w,h = 256, 256\n",
    "epochs = 10\n",
    "model_save_dir = './trained_models'\n",
    "# ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0156fb3-9595-4916-abd9-e8a72334d814",
   "metadata": {},
   "source": [
    "## Dataset pipeline\n",
    "\n",
    "The following dataset pipeline is using an augmented dataset generated from manually annotated images - all source images were annotated using the VGG Image Annotator, or VIA. Here, we have simply used tensorflow's dataset pipeline to batch process this dataset out-of-core to overcome memory limitations on our system. Note that images are rescaled by default in the model so images can be supplied in uint8 format, though masks must be normalised to 0 - 1 scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31b8ce-ea51-4daf-a7e0-c9b9fbfef00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep ------------------------------------------------------\n",
    "def read_img(im, an):\n",
    "    img = tf.io.read_file(im)\n",
    "    img = tf.image.decode_png(img, channels=1)\n",
    "    img.set_shape([None, None, 1])\n",
    "    img = tf.image.resize_with_pad(img, 256, 256)\n",
    "\n",
    "    ann = tf.io.read_file(an)\n",
    "    ann = tf.image.decode_png(ann, channels=1)\n",
    "    ann.set_shape([None, None, 1])\n",
    "    ann = tf.image.resize_with_pad(ann, 256, 256)\n",
    "\n",
    "    return img, ann\n",
    "\n",
    "def dataset(img_files, annot_files, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_files, annot_files))\n",
    "    dataset = dataset.map(read_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "train_images = glob.glob('./train_images/*.png')\n",
    "random.shuffle(train_images)\n",
    "train_annotations = [re.sub('./train_images/', './train_annotations/', f) for f in train_images]\n",
    "\n",
    "val_images = glob.glob('./val_images/*.png')\n",
    "val_annotations = [re.sub('./val_images/', './val_annotations/', f) for f in val_images]\n",
    "\n",
    "test_images = glob.glob('./test_images/*.png')\n",
    "test_annotations = [re.sub('./test_images/', './test_annotations/', f) for f in test_images]\n",
    "\n",
    "train_data = dataset(train_images, train_annotations, batch_size)\n",
    "val_data = dataset(val_images, val_annotations, batch_size)\n",
    "test_data = dataset(test_images, test_annotations, batch_size)\n",
    "\n",
    "# Display a single example\n",
    "for b in train_data:\n",
    "    im, ann = b\n",
    "\n",
    "    for i,a in zip(im, ann):\n",
    "        print(i.numpy().ptp(), a.numpy().ptp())\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        ax1.imshow(i, cmap='gray')\n",
    "        ax2.imshow(a, cmap='gray')\n",
    "        ax1.set_title('Input Image')\n",
    "        ax2.set_title('Ground Truth Mask')\n",
    "        plt.show()\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7836a46-4090-4df0-a72d-86de93a51901",
   "metadata": {},
   "source": [
    "## Training and evaluation\n",
    "\n",
    "This is the main training loop for constructing, training and computing summary metrics for each model. For each model we use the Adam optimizer as well as Binary Cross Entropy for loss given that this is a binary segmentation problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debfa6f2-ceab-46ba-a5af-d4bec32157bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training -------------------------------------------------\n",
    "model_stats = dict(\n",
    "    name=[],\n",
    "    loss=[],\n",
    "    val_loss=[],\n",
    "    test_loss=[],\n",
    "    binary_io_u=[],\n",
    "    val_binary_io_u=[],\n",
    "    test_binary_io_u=[],\n",
    "    params=[],\n",
    "    total_train_time=[],\n",
    "    per_epoch_time=[],\n",
    "    per_step_time=[]\n",
    ")\n",
    "\n",
    "model_defs = ['UNet', 'UNet2plus', 'UNet3plus', 'SegNet', 'FCN', 'PSPNet', 'DeepLabV3', 'HRNetV2']\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "\n",
    "for model_name in model_defs:\n",
    "    print('---------------------------------')\n",
    "    print(f'Training: {model_name}')\n",
    "    print('---------------------------------')        \n",
    "        \n",
    "    MODEL = build_model(input_shape=(w, h, 1), n_classes=1, model=model_name)\n",
    "\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    MODEL.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss=loss,\n",
    "        metrics=[keras.metrics.BinaryIoU([0, 1], 0.8)],\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f'./trained_models/{model_name}_lymnaea_binary.h5',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train the model, doing validation at the end of each epoch.\n",
    "    start = time.time()\n",
    "    history = MODEL.fit(\n",
    "        train_data,\n",
    "        epochs=epochs, \n",
    "        callbacks=callbacks,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=val_data)\n",
    "    end = time.time()\n",
    "\n",
    "    MODEL.load_weights(f'./trained_models/{model_name}_lymnaea_binary.h5')\n",
    "    \n",
    "    test_loss, test_accuracy = MODEL.evaluate(test_data)\n",
    "    print(MODEL.count_params())\n",
    "    \n",
    "    model_stats['name'].append('UNet')\n",
    "    model_stats['loss'].append(max(history.history['loss']))\n",
    "    model_stats['val_loss'].append(max(history.history['val_loss']))\n",
    "    model_stats['test_loss'].append(test_loss)\n",
    "    model_stats['binary_io_u'].append(max(history.history['binary_io_u']))\n",
    "    model_stats['val_binary_io_u'].append(max(history.history['val_binary_io_u']))\n",
    "    model_stats['test_binary_io_u'].append(test_accuracy)\n",
    "    model_stats['params'].append(MODEL.count_params())\n",
    "    model_stats['total_train_time'].append(end - start)\n",
    "    model_stats['per_epoch_time'].append((end - start) / epochs)\n",
    "    model_stats['per_step_time'].append(((end - start) / epochs) / len(train_data))\n",
    "    \n",
    "    ax1.plot(history.history['loss'], label=model_name)\n",
    "    ax2.plot(history.history['val_loss'], label=model_name)\n",
    "    ax1.set_title('Training loss')\n",
    "    ax2.set_title('Validation loss')\n",
    "    \n",
    "    ax3.plot(history.history['binary_io_u'], label=model_name)\n",
    "    ax4.plot(history.history['val_binary_io_u'], label=model_name)\n",
    "    ax3.set_title('Training Binary IoU')\n",
    "    ax4.set_title('Validation Binary IoU')\n",
    "    \n",
    "    del MODEL\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "df = pd.DataFrame(data=model_stats)\n",
    "df.to_csv('./model_stats_lymnaea.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
